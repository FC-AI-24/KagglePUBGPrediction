{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-15T10:02:13.500929Z","iopub.execute_input":"2022-06-15T10:02:13.501418Z","iopub.status.idle":"2022-06-15T10:02:13.519523Z","shell.execute_reply.started":"2022-06-15T10:02:13.501329Z","shell.execute_reply":"2022-06-15T10:02:13.518181Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:02:13.521217Z","iopub.execute_input":"2022-06-15T10:02:13.521674Z","iopub.status.idle":"2022-06-15T10:02:14.229437Z","shell.execute_reply.started":"2022-06-15T10:02:13.521637Z","shell.execute_reply":"2022-06-15T10:02:14.228352Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 데이터 불러오기\nsample_path = '../input/pubg-finish-placement-prediction/sample_submission_V2.csv' \ntrain_path = '../input/pubg-finish-placement-prediction/train_V2.csv'\ntest_path = '../input/pubg-finish-placement-prediction/test_V2.csv'\nsample = pd.read_csv(sample_path)\ntrain= pd.read_csv(train_path)\ntest=pd.read_csv(test_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:02:14.231145Z","iopub.execute_input":"2022-06-15T10:02:14.231537Z","iopub.status.idle":"2022-06-15T10:02:57.602007Z","shell.execute_reply.started":"2022-06-15T10:02:14.231502Z","shell.execute_reply":"2022-06-15T10:02:57.600863Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#test_o는 불러온 데이터의 원본을 복사, df_o는 불러온 train데이터의 원본을 복사하고 타겟벨류의 결측치 제거, df는 train에 활용하기 위해 복사\ntest_o=test.copy()\ndf_o= pd.read_csv(train_path)\ndf_o= df_o.dropna()\ndf=df_o.copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:02:57.605164Z","iopub.execute_input":"2022-06-15T10:02:57.605734Z","iopub.status.idle":"2022-06-15T10:03:25.498090Z","shell.execute_reply.started":"2022-06-15T10:02:57.605681Z","shell.execute_reply":"2022-06-15T10:03:25.496986Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#데이터 타입 최적화-> 메모리 최적화\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\ndf = reduce_mem_usage(df)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:03:25.499587Z","iopub.execute_input":"2022-06-15T10:03:25.500681Z","iopub.status.idle":"2022-06-15T10:03:30.148864Z","shell.execute_reply.started":"2022-06-15T10:03:25.500632Z","shell.execute_reply":"2022-06-15T10:03:30.147769Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#함수로 만들자\ndef feature_engineering(df):\n    #수치형 데이터\n    df_col= [ 'assists', 'boosts', 'damageDealt', 'DBNOs',\n       'headshotKills', 'heals', 'killPlace', 'killPoints', 'kills','matchType',\n       'killStreaks', 'longestKill', 'matchDuration', 'maxPlace',\n       'numGroups', 'rankPoints', 'revives', 'rideDistance', 'roadKills',\n       'swimDistance', 'teamKills', 'vehicleDestroys', 'walkDistance',\n       'weaponsAcquired', 'winPoints']\n    # df_t는 데이터를 그룹아이디 기준으로 평균을 내기 위해 저장할 데이터프레임\n    df_t=df.copy()\n    #라벨인코딩\n    le = LabelEncoder()\n    df_t['matchType']= le.fit_transform(df_t['matchType'])\n    df_t['matchType']\n    # 그룹아이디를 기준으로 수치형 컬럼들 평균화\n    df_t= df_t.groupby(['groupId'])[df_col].agg('mean').reset_index()\n\n\n    \n    return df_t\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:03:30.150247Z","iopub.execute_input":"2022-06-15T10:03:30.150590Z","iopub.status.idle":"2022-06-15T10:03:30.158301Z","shell.execute_reply.started":"2022-06-15T10:03:30.150559Z","shell.execute_reply":"2022-06-15T10:03:30.157278Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#df에는 앞에서 만든 함수를 통해 피처 엔지니어링 수치형 데이터를 평균을 냄\ndf = feature_engineering(df)\ntest = feature_engineering(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:03:30.159921Z","iopub.execute_input":"2022-06-15T10:03:30.160472Z","iopub.status.idle":"2022-06-15T10:03:55.566919Z","shell.execute_reply.started":"2022-06-15T10:03:30.160435Z","shell.execute_reply":"2022-06-15T10:03:55.565634Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#원본 train 데이터의 순서와 로우를 잃지 않기 위해 그룹아이디와 그에 맞는 타겟벨류를 가지고옴\ntrain_t=df_o.copy()\ntrain_t=train_t[['groupId','winPlacePerc']]\n# 데이터 합치기- 앞에서 데이터를 피처 엔지니어링한 평균을 원본 train 데이터의 순서에 맞게 넣기 위해 merge를 사용 how='left'는 \n#두 데이터프레임중 기준을 잡을 데이터프레임을 정함, 원본train의 순서와 로우수를 맞춰줌\ntrain_t=pd.merge(train_t,df,how='left',on='groupId')\n# 모델 학습을 위해 두가지 변수를 제거 killplace는 큰 비중을 차지하여 공부를 위해 제거 groupId는 학습을 위해 수치형 데이터만\n#남게 하기 위해 제거 \ntrain_t=train_t.drop(columns=['killPlace','groupId'])\ntrain_t\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:03:55.568414Z","iopub.execute_input":"2022-06-15T10:03:55.569052Z","iopub.status.idle":"2022-06-15T10:04:12.688888Z","shell.execute_reply.started":"2022-06-15T10:03:55.569004Z","shell.execute_reply":"2022-06-15T10:04:12.687856Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#원본 데이터의 순서와 로우 수를 위해 그룹아이디를 가지고옴\ntest_t=test_o.copy()\ntest_t=test_t[['groupId']]\n#데이터 합치기- 위와 동일\ntest_t=pd.merge(test_t,test,how='left',on='groupId')\ntest_t=test_t.drop(columns=['killPlace','groupId'])\ntest_t","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:04:12.690583Z","iopub.execute_input":"2022-06-15T10:04:12.692352Z","iopub.status.idle":"2022-06-15T10:04:18.208892Z","shell.execute_reply.started":"2022-06-15T10:04:12.692293Z","shell.execute_reply":"2022-06-15T10:04:18.207913Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#모델링을 위해 winplacePerc를 제거하고, y로 값을 따로 만듬\nx=train_t.drop(columns='winPlacePerc')\ny=train_t[['winPlacePerc']]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:04:18.210378Z","iopub.execute_input":"2022-06-15T10:04:18.210858Z","iopub.status.idle":"2022-06-15T10:04:18.489128Z","shell.execute_reply.started":"2022-06-15T10:04:18.210812Z","shell.execute_reply":"2022-06-15T10:04:18.488089Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#스케일링, test_t도 스케일링을 함께 함\nscaler = StandardScaler()\ntest_t=scaler.fit_transform(test_t)\nx=scaler.fit_transform(x)\nx_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2,random_state=0)\nx_train.shape,x_val.shape,y_train.shape,y_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:04:18.490756Z","iopub.execute_input":"2022-06-15T10:04:18.491252Z","iopub.status.idle":"2022-06-15T10:04:23.971965Z","shell.execute_reply.started":"2022-06-15T10:04:18.491204Z","shell.execute_reply":"2022-06-15T10:04:23.971083Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# lightgbm 활용-> 학습용\nimport lightgbm as lgb\nd_train = lgb.Dataset(x_train, label=y_train)\nd_val = lgb.Dataset(x_val,label=y_val)\nparams = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':1000, 'early_stopping_rounds':200,\n              \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.7,\n               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n             }\n\nclf = lgb.train(params, d_train, valid_sets=[d_train, d_val],early_stopping_rounds=200, verbose_eval=1000)\n# pred = clf.predict(x_test, num_iteration=clf.best_iteration)\n#mae=mean_absolute_error(y_test,pred)\nprint(clf)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:04:23.973395Z","iopub.execute_input":"2022-06-15T10:04:23.973690Z","iopub.status.idle":"2022-06-15T10:07:34.403328Z","shell.execute_reply.started":"2022-06-15T10:04:23.973663Z","shell.execute_reply":"2022-06-15T10:07:34.402086Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 학습한 모델 clf로 테스트파일을 예측함 pred는 테스트 파일의 y벨류가 들어있음\npred = clf.predict(test_t, num_iteration=clf.best_iteration)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:07:34.406046Z","iopub.execute_input":"2022-06-15T10:07:34.407311Z","iopub.status.idle":"2022-06-15T10:08:34.910034Z","shell.execute_reply.started":"2022-06-15T10:07:34.407266Z","shell.execute_reply":"2022-06-15T10:08:34.908859Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 제출을 위해 파일 변환\ndf1 = pd.read_csv('../input/pubg-finish-placement-prediction/sample_submission_V2.csv')\ndf1['winPlacePerc']=pred\ndf1.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:08:34.911608Z","iopub.execute_input":"2022-06-15T10:08:34.915750Z","iopub.status.idle":"2022-06-15T10:08:43.323367Z","shell.execute_reply.started":"2022-06-15T10:08:34.915695Z","shell.execute_reply":"2022-06-15T10:08:43.322075Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}